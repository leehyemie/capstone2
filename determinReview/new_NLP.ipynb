{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('review.txt')\n",
    "test_data = pd.read_table('review.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1735, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'].nunique(), train_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 열의 중복 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-4419b0600db1>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>친절하시고 깔끔하고 좋았습니다</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>조용하고 고기도 굿</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>갈비탕과 냉면 육회비빔밥이 맛있습니다</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>대체적으로 만족하나와인의 구성이 살짝 아쉬움</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>고기도 맛있고 서비스는 더 최고입니다</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                  document  label\n",
       "0   1          친절하시고 깔끔하고 좋았습니다    1.0\n",
       "1   2                조용하고 고기도 굿    1.0\n",
       "2   3      갈비탕과 냉면 육회비빔밥이 맛있습니다    1.0\n",
       "3   4  대체적으로 만족하나와인의 구성이 살짝 아쉬움    1.0\n",
       "4   5      고기도 맛있고 서비스는 더 최고입니다    1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-39af766def0e>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "\n",
    "train_data = train_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1733/1733 [00:05<00:00, 306.56it/s]\n",
      "c:\\Users\\sk971\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "okt = Okt()\n",
    "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)\n",
    "\n",
    "X_train = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "y_train = np.array(train_data['label'])\n",
    "\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  count = 0\n",
    "  for sentence in nested_list:\n",
    "    if(len(sentence) <= max_len):\n",
    "        count = count + 1\n",
    "\n",
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "22/22 [==============================] - 3s 47ms/step - loss: 0.4647 - acc: 0.8149 - val_loss: 0.6907 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.68497, saving model to best_model.h5\n",
      "Epoch 2/15\n",
      "22/22 [==============================] - 1s 28ms/step - loss: 0.3610 - acc: 0.8525 - val_loss: 0.4788 - val_acc: 0.7486\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.68497 to 0.74855, saving model to best_model.h5\n",
      "Epoch 3/15\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.2350 - acc: 0.9053 - val_loss: 0.5633 - val_acc: 0.7861\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74855 to 0.78613, saving model to best_model.h5\n",
      "Epoch 4/15\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 0.1760 - acc: 0.9320 - val_loss: 0.3672 - val_acc: 0.8353\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78613 to 0.83526, saving model to best_model.h5\n",
      "Epoch 5/15\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.1360 - acc: 0.9544 - val_loss: 0.4447 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.83526 to 0.84682, saving model to best_model.h5\n",
      "Epoch 6/15\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.1131 - acc: 0.9581 - val_loss: 0.3412 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.84682\n",
      "Epoch 7/15\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.0933 - acc: 0.9696 - val_loss: 0.4694 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.84682\n",
      "Epoch 8/15\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.0709 - acc: 0.9725 - val_loss: 0.6122 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.84682\n",
      "Epoch 9/15\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.0618 - acc: 0.9776 - val_loss: 0.3836 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.84682 to 0.86127, saving model to best_model.h5\n",
      "Epoch 10/15\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.0501 - acc: 0.9812 - val_loss: 0.6009 - val_acc: 0.8382\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86127\n",
      "Epoch 00010: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "\n",
    "loaded_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "  new_sentence = str(new_sentence).replace(\"\\n\",\"\")\n",
    "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "  score = float(loaded_model.predict(pad_new)) # 예측\n",
    "  if(score > 0.5):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data5.csv',encoding='utf-8',names=['names','menu','review','loc','review_num','score'])\n",
    "\n",
    "review_df = df['review']\n",
    "df = df.astype({'score':'float'})\n",
    "\n",
    "i = 0\n",
    "for line in review_df:\n",
    "    df.loc[i,'score'] = sentiment_predict(line)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "df = df.drop(['review'],axis=1)\n",
    "df['avg_score'] = df.groupby(['names']).transform('mean')\n",
    "\n",
    "df = df.drop_duplicates(subset=['names'])\n",
    "df.drop(['score'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('data_5.csv',encoding='utf-8-sig')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b093208ce00546bba176cacd9e75f499eb1d011fc5f25a2ad2eba6a6f60f9a99"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
